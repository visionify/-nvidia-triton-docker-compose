version: "3.0"

services:
  visionai-triton:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    container_name: visionai-triton
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    volumes:
      - ${MODEL_REPO}:/models
    command: ["tritonserver", "--model-store=/models"]
    shm_size: 2g
    ulimits:
      memlock: -1
      stack: 67108864
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
